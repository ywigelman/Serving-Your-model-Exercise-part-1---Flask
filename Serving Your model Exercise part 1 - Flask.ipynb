{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving your model exercise. Part 1 - Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "Reminder, there will usually be 3 different places where the code relevant to our model prediction runs:\n",
    "1. **Training computer / server** - where we train our model and save it\n",
    "2. **Inference server** - server that listens to REST API requests to make predictions / inferences with the model that was trained on the model server. Potentially, we could have many such servers. \n",
    "3. **Client** - client application (browser, mobile app etc.) that needs a prediction, and requests from **inference server** over HTTP with REST API to make the prediction\n",
    "\n",
    "There are 2 directions for **sending data**:\n",
    "1. Data sent via a REST API from the **client** to the **Inference server**\n",
    "1. Prediction result that's sent from the **Inference server** back to the **client**\n",
    "\n",
    "There are 2 ways to send the data from **client** to the **Inference server**:\n",
    "1. As parameters of the URL\n",
    "1. As a body of the HTTP request\n",
    "\n",
    "Prediction that's returned from the **Inference server** to the **client** will always be in the body of the HTTP (limitation of the HTTP protocol).  However, it could be in different formats - regular string, HTML page, or a JSON file.\n",
    "\n",
    "In this exercise we will learn about 2 combinations of the above:\n",
    "1. Sending data as **parameters of the request** (as part of the URL) and receiving data as a **regular string** - useful for small and short data as inputs to the model, and when the prediction is short / simple.  We will use it to implement a **single prediction API**.\n",
    "1. Sending data in the HTTP body as a **JSON file**, and getting back the prediction as a **JSON file** - more relevant for when the data has many features / complicated features, and when the prediction response is itself slightly longer / more complicated.  We will use it to implement **multiple predictions API**\n",
    "\n",
    "Of course, there is no connection between the format of data sent to the server, and received back from the server, so you could have other variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting to a trained model\n",
    "- Choose one of the models you trained in one the previous exercises or any other model. **Do not take something from many Flask examples online!**  **For easy debugging** - It's better to use a model with a small number of features and where the feature values are not long arrays (you can also take a small subsete of features of existing model).\n",
    "- Specify where can one download the dataset from (to be used during checking the exercise)\n",
    "- Say in one word what is the business problem and what you are predicting \n",
    "- Preprocess, split to train and test dataset\n",
    "- Train the model - how well your models predicts (accuracy / $R^2$) is not of big importance here\n",
    "- Do a few predictions of the model locally\n",
    "\n",
    "This code will run on the **training server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save you model, predict with saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate in this notebook code that will happen during training on the **training server**:\n",
    "- Using `pickle`, save your model to disk. Reference: https://scikit-learn.org/stable/modules/model_persistence.html\n",
    "- Save the test dataset to file.  What's a good format(s) for saving datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate in this notebook code that will happen during inference on the **inference server**:\n",
    "- Load the model again with `pickle`.\n",
    "- Read the test dataset file, and perform some predictions\n",
    "- Compare the predictions received before saving the model, and after reading a saved model.  Show that you get the same results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Serve your model - using URL parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with **training server**, since we have the saved model.  From now all that's relevant is **inference server** and **client** code.\n",
    "\n",
    "Let's create the **inference server** that answers to REST APIs with predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `flask`, create a Pycharm project and implement the following prediction API:\n",
    "- **Single prediction API** that receives inputs as parameters (no body), and returns a single prediction as a string / text.  \n",
    "- Example: http://localhost:5000/predict_single?key1=value1&key2=value2 (replace `key1`, `value1` etc. names with your relevant feature names and values) that would return the class label (example: `0` / `1`)\n",
    "- **Important:** For efficiency purposes, consider what's the best place in your code to put the code that reads the model.  Why?\n",
    "- **Important:** In general, take runtime efficiency into account.  Your API might be called large number of times per second, and you will be paying for more inference servers if your code is not efficient.\n",
    "- Copy your **inference server** code also here for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consume your model with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate client requests for inference / prediction:\n",
    "Assume your client runs Python code also, and not only your training and inference servers (in real case scenario, often times your client code will actually not be in Python).\n",
    "Use Python `requests` module from here to request a prediction by the client from to the inference .  To pass parameters with Python `requests` module, use the `params` parameter of `requests.get` API.\n",
    "\n",
    "**Print input and output of the prediction.**\n",
    "\n",
    "**Warning**: don't get used to seeing it in a Jupyter notebook.  This code will usually run inside a **client application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Serve your model - using JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `flask`, add code to your previous file in Pycharm **with inference code** to create the following prediction API (in addition to **Single prediction API** done above):\n",
    "- **Multiple prediction API** that receives input many observations to predict on as a json file in the body, and returns a json file with predictions.\n",
    "- Your **JSON** file format has to be efficient, clear and following JSON file syntax: \n",
    "  - JSON file is a nested structure of potentially multiple dictionaries and lists \n",
    "  - JSON file tip: Use lists, every member in the list can be a dictionary of all the features.  \n",
    "  - JSON file tip: Do not put indexes of predictions into the JSON files, indexes of predictions can be easily computed with Python code later \n",
    "  - JSON files are sometimes slightly verbose, but are extremely human readable.  Just looking at your JSON files of input and output, is it possible to understand what were the observations in input and what were the predictions in output?\n",
    "  - See https://www.json.org/json-en.html for JSON format\n",
    "- Think about efficiency of your code - your REST API might be called a huge number of times, with a huge number of observations every time.  Can part of the code be done only once?  Can you predict on everything together? Can you do less or cheaper data conversions?\n",
    "- Example of URL that will be used to predict: http://localhost:5000/car-price\n",
    "- Reference for working with JSONs in Flask: https://pythonise.com/series/learning-flask/working-with-json-in-flask\n",
    "- Do you need a GET or a POST type of REST API call? Does it change what you did in step 3?  Conceptually, would you say it makes sense to use GET or POST types for predictions?\n",
    "- Copy your **inference server** code also here for reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python `requests` module from here to make a prediction, and **print the input, and the output** of the prediction (or part of it if it's too large).  \n",
    "\n",
    "**Hint:** to pass a JSON file to the `requests` module, use `json` parameter of the `requests.post` API.\n",
    "\n",
    "**Warning**: don't get used to seeing it in a Jupyter notebook.  This code will usually run inside a **client application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Submit a zip file with:\n",
    "1. This notebook\n",
    "2. Your Python inference server file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
